{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam/ham predictor using scikit-learn (sklearn)\n",
    "Supervised classification algorithm\n",
    "\n",
    "## Basic flow:\n",
    "**Load dataset**   \n",
    "**Feature extraction**   \n",
    "**Classifier**   \n",
    "**Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) load dataset\n",
    "\n",
    "- Load messages dataset using pandas library, which is a popular library for data manipulation and analysis.\n",
    "- Split dataset into training and testing set (sklearn train_test_split function do it in a pretty simple way by specifying test size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages: 5572\n",
      "Training set contains 4457 messages\n",
      "Testing set contains 1115 messages\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    messages  = pandas.read_csv(\"spam.csv\", encoding='latin-1')\n",
    "    messages = messages.rename(columns={\"v1\":\"label\", \"v2\":\"content\"})\n",
    "   \n",
    "    contents_train, contents_test, labels_train, labels_test = train_test_split(messages['content'],\n",
    "                                                                                messages['label'],\n",
    "                                                                                test_size=0.2)\n",
    "        \n",
    "    print \"Total messages:\", messages.shape[0]\n",
    "    print \"Training set contains\", contents_train.shape[0], \"messages\"\n",
    "    print \"Testing set contains\", contents_test.shape[0], \"messages\"\n",
    "    \n",
    "    return contents_train, contents_test, labels_train, labels_test\n",
    "\n",
    "contents_train, contents_test, labels_train, labels_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatisation\n",
    "def split_into_lemmas(content):\n",
    "    words = TextBlob(content.lower()).words\n",
    "    return [word.lemma for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) feature extraction \n",
    "\n",
    "- Tokenization, lemmatization and stop words removal\n",
    "- Convert messages into a vector that machine learning models can understand: CountVectorizer convert a collection of text documents to a matrix of token counts\n",
    "- fit_transform method does two things: it learns the vocabulary of the messages and extracts word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(contents_train, contents_test):\n",
    "    count_vector = CountVectorizer(analyzer=split_into_lemmas, stop_words='english')\n",
    "    \n",
    "    #Learn the vocabulary dictionary and return term-document matrix.\n",
    "    train_messages = count_vector.fit_transform(contents_train)\n",
    "    test_messages = count_vector.transform(contents_test)\n",
    "    \n",
    "    return train_messages, test_messages\n",
    "\n",
    "train_messages, test_messages = feature_extraction(contents_train, contents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve accuracy, we can use word frequencies instead of word count occurances. Instead of how many times a word appears in the message, we will compute the \"percentage\" of the message that is made by the word.\n",
    "Most popular method is called TF-IDF. \n",
    "\n",
    "Term Frequency: How often a given word appears in a message.\n",
    "Inverse Document Frequency: This downscales words that appear a lot across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_messages = TfidfTransformer().fit_transform(train_messages)\n",
    "tfidf_test_messages = TfidfTransformer().fit_transform(test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(classifier, predictions, labels_test):\n",
    "\n",
    "    metric_matrix = confusion_matrix(labels_test, predictions)\n",
    "    print classifier + (' classifier accuracy: '),format(accuracy_score(labels_test, predictions))\n",
    "    #False positive\n",
    "    print format(metric_matrix[0][1]),('ham messages were wrongly classified as spam while'),format(metric_matrix[0][0]),('were classified correclty')\n",
    "    #False negative\n",
    "    print format(metric_matrix[1][0]), ('spam messages wrongly classified as ham while'), format(metric_matrix[1][1]),('were classified correclty')\n",
    "    #print classification_report(labels_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) naive bayes classifier \n",
    "\n",
    "Based on Bayes probability theorem assuming independence between every pair of features, the probability of a message being spam(ham) given it contains a word:    \n",
    "    \n",
    "\\begin{align}\n",
    "P(S|W)=\\frac{P(W|S)P(S)}{P(W)} = \\frac{P(W|S)P(S)}{P(W|S)P(S)+P(W|H)P(H)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(train_messages, labels_train, test_messages):\n",
    "    naive_bayes = MultinomialNB() \n",
    "    #train the classifier based on the training messages\n",
    "    naive_bayes.fit(train_messages, labels_train) \n",
    "    \n",
    "    #predic label of test messages using the trained model\n",
    "    nb_predictions = naive_bayes.predict(test_messages) \n",
    "    return nb_predictions\n",
    "\n",
    "nb_predictions = naive_bayes_classifier(train_messages, labels_train, test_messages)\n",
    "nb_tfidf_predictions = naive_bayes_classifier(tfidf_train_messages, labels_train, tfidf_test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) NB metrics report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classifier accuracy:  0.983856502242\n",
      "8 ham messages were wrongly classified as spam while 968 were classified correclty\n",
      "10 spam messages wrongly classified as ham while 129 were classified correclty\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Naive Bayes with TFIDF classifier accuracy:  0.956053811659\n",
      "0 ham messages were wrongly classified as spam while 976 were classified correclty\n",
      "49 spam messages wrongly classified as ham while 90 were classified correclty\n",
      "\n",
      "------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes classifier\n",
    "metrics('Naive Bayes', nb_predictions, labels_test)\n",
    "print '\\n------------------------------------------------\\n'\n",
    "\n",
    "#Naive Bayes classifier with TFIDF\n",
    "metrics('Naive Bayes with TFIDF',nb_tfidf_predictions, labels_test)\n",
    "\n",
    "print '\\n------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) SVM classifier\n",
    "\n",
    "Treat each data item is a point in n-dimensional space with the value of each feature being the value of a particular coordinate. Classification is performed by finding the hyper-plane that differentiate two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svc_classifier(train_messages, labels_train, test_messages):\n",
    "    linear_svc = LinearSVC()\n",
    "    #train the classifier based on the training messages\n",
    "    linear_svc.fit(train_messages, labels_train)\n",
    "    \n",
    "    #predic label of test messages using the trained model\n",
    "    linear_svc_predictions = linear_svc.predict(test_messages)\n",
    "    return linear_svc_predictions\n",
    "\n",
    "linear_svc_predictions = linear_svc_classifier(train_messages, labels_train, test_messages)\n",
    "linear_svc_tfidf_predicitions = linear_svc_classifier(tfidf_train_messages, labels_train, tfidf_test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) SVM metrics report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC classifier accuracy:  0.985650224215\n",
      "2 ham messages were wrongly classified as spam while 974 were classified correclty\n",
      "14 spam messages wrongly classified as ham while 125 were classified correclty\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Linear SVC with TDIDF classifier accuracy:  0.987443946188\n",
      "1 ham messages were wrongly classified as spam while 975 were classified correclty\n",
      "13 spam messages wrongly classified as ham while 126 were classified correclty\n",
      "\n",
      "------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear SVC classifier\n",
    "metrics('Linear SVC', linear_svc_predictions, labels_test)\n",
    "print '\\n------------------------------------------------\\n'\n",
    "\n",
    "#Linear SVC classifier with TFIDF\n",
    "metrics('Linear SVC with TDIDF', linear_svc_tfidf_predicitions, labels_test)\n",
    "print '\\n------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements \n",
    "** Use sklearn pipeline ** \n",
    "** Add ngram_range parameter **\n",
    "** k-fold cross-validation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
